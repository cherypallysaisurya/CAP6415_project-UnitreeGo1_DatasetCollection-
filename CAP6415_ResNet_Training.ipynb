{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0e6ddce",
   "metadata": {},
   "source": [
    "## Step 1: Setup and GPU Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d98389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\nUsing device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ba84fd",
   "metadata": {},
   "source": [
    "## Step 2: Upload Your Dataset\n",
    "\n",
    "Upload your `dataset_v2.zip` file containing the 5 session folders.\n",
    "\n",
    "Each session folder should have:\n",
    "- `frames/` - camera images (PNG)\n",
    "- `velodyne/` - LiDAR point clouds (BIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e4ec02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload dataset zip file\n",
    "from google.colab import files\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "print(\"Please upload your dataset_v2.zip file...\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Extract the zip file\n",
    "zip_filename = list(uploaded.keys())[0]\n",
    "print(f\"\\nExtracting {zip_filename}...\")\n",
    "\n",
    "with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
    "    zip_ref.extractall('data')\n",
    "\n",
    "print(\"Extraction complete!\")\n",
    "print(\"\\nContents:\")\n",
    "!ls -la data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5f7be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the dataset directory (handle nested folders)\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Look for session folders\n",
    "data_root = Path('data')\n",
    "\n",
    "# Find folders that contain 'frames' and 'velodyne' subdirectories\n",
    "def find_sessions(root):\n",
    "    sessions = []\n",
    "    for path in root.rglob('*'):\n",
    "        if path.is_dir():\n",
    "            if (path / 'frames').exists() and (path / 'velodyne').exists():\n",
    "                sessions.append(path)\n",
    "    return sessions\n",
    "\n",
    "session_dirs = find_sessions(data_root)\n",
    "print(f\"Found {len(session_dirs)} sessions:\")\n",
    "for s in session_dirs:\n",
    "    num_frames = len(list((s / 'frames').glob('*.png')))\n",
    "    num_lidar = len(list((s / 'velodyne').glob('*.bin')))\n",
    "    print(f\"  {s.name}: {num_frames} images, {num_lidar} LiDAR files\")\n",
    "\n",
    "# Set DATA_DIR to parent of sessions\n",
    "if session_dirs:\n",
    "    DATA_DIR = session_dirs[0].parent\n",
    "    print(f\"\\nDATA_DIR set to: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb17119",
   "metadata": {},
   "source": [
    "## Step 3: Define Dataset and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ef6248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Session configuration\n",
    "TRAIN_SESSIONS = [\n",
    "    \"4th_floor_hallway_20251206_132136\",\n",
    "    \"4th_floor_lounge_20251206_154822\",\n",
    "    \"5th_floor_hallway_20251206_161536\",\n",
    "    \"3rd_floor_hallway_20251206_162223\",\n",
    "]\n",
    "\n",
    "TEST_SESSIONS = [\n",
    "    \"Mlab_20251207_112819\",\n",
    "]\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 15\n",
    "LEARNING_RATE = 0.001\n",
    "IMAGE_SIZE = 224\n",
    "\n",
    "print(\"Configuration loaded!\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Image size: {IMAGE_SIZE}x{IMAGE_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3948545c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CameraLiDARDataset(Dataset):\n",
    "    \"\"\"Dataset that pairs camera images with LiDAR-derived targets.\"\"\"\n",
    "    \n",
    "    def __init__(self, session_names, data_dir, transform=None):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.transform = transform\n",
    "        self.samples = []  # List of (image_path, target)\n",
    "        \n",
    "        for session_name in session_names:\n",
    "            # Find session directory (may be nested)\n",
    "            session_dir = None\n",
    "            for path in self.data_dir.rglob(session_name):\n",
    "                if path.is_dir() and (path / 'frames').exists():\n",
    "                    session_dir = path\n",
    "                    break\n",
    "            \n",
    "            if session_dir is None:\n",
    "                print(f\"  Warning: Session {session_name} not found\")\n",
    "                continue\n",
    "            \n",
    "            image_dir = session_dir / \"frames\"\n",
    "            velodyne_dir = session_dir / \"velodyne\"\n",
    "            \n",
    "            if not image_dir.exists() or not velodyne_dir.exists():\n",
    "                print(f\"  Warning: Missing data in {session_name}\")\n",
    "                continue\n",
    "            \n",
    "            # Get all image files\n",
    "            image_files = sorted(image_dir.glob(\"*.png\"))\n",
    "            \n",
    "            for img_path in image_files:\n",
    "                frame_id = img_path.stem\n",
    "                lidar_path = velodyne_dir / f\"{frame_id}.bin\"\n",
    "                \n",
    "                if lidar_path.exists():\n",
    "                    # Load LiDAR and compute target (mean distance)\n",
    "                    points = np.fromfile(str(lidar_path), dtype=np.float32).reshape(-1, 5)\n",
    "                    x, y, z = points[:, 0], points[:, 1], points[:, 2]\n",
    "                    mean_distance = np.sqrt(x**2 + y**2 + z**2).mean()\n",
    "                    \n",
    "                    self.samples.append((img_path, mean_distance))\n",
    "        \n",
    "        print(f\"  Loaded {len(self.samples)} samples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, target = self.samples[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, torch.tensor(target, dtype=torch.float32)\n",
    "\n",
    "\n",
    "class ResNetRegressor(nn.Module):\n",
    "    \"\"\"ResNet18 modified for regression.\"\"\"\n",
    "    \n",
    "    def __init__(self, pretrained=True):\n",
    "        super(ResNetRegressor, self).__init__()\n",
    "        \n",
    "        # Load pretrained ResNet18\n",
    "        self.resnet = models.resnet18(weights='IMAGENET1K_V1' if pretrained else None)\n",
    "        \n",
    "        # Replace final FC layer for regression\n",
    "        num_features = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Sequential(\n",
    "            nn.Linear(num_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.resnet(x).squeeze()\n",
    "\n",
    "print(\"Dataset and Model classes defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f15e8c",
   "metadata": {},
   "source": [
    "## Step 4: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaa7b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(\"Loading training data...\")\n",
    "train_dataset = CameraLiDARDataset(TRAIN_SESSIONS, DATA_DIR, transform=train_transform)\n",
    "\n",
    "print(\"\\nLoading test data...\")\n",
    "test_dataset = CameraLiDARDataset(TEST_SESSIONS, DATA_DIR, transform=test_transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(\"TRAIN/TEST SPLIT\")\n",
    "print(\"=\"*50)\n",
    "total = len(train_dataset) + len(test_dataset)\n",
    "print(f\"Train: {len(train_dataset)} samples ({100*len(train_dataset)/total:.1f}%)\")\n",
    "print(f\"Test:  {len(test_dataset)} samples ({100*len(test_dataset)/total:.1f}%)\")\n",
    "\n",
    "# Target statistics\n",
    "train_targets = [t for _, t in train_dataset.samples]\n",
    "test_targets = [t for _, t in test_dataset.samples]\n",
    "print(f\"\\nTrain distance range: [{min(train_targets):.2f}, {max(train_targets):.2f}] meters\")\n",
    "print(f\"Test distance range:  [{min(test_targets):.2f}, {max(test_targets):.2f}] meters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09dbb31",
   "metadata": {},
   "source": [
    "## Step 5: Visualize Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571d66a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample images with their targets\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < len(train_dataset):\n",
    "        img_path, target = train_dataset.samples[i * 100]  # Sample every 100th\n",
    "        img = Image.open(img_path)\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(f\"Distance: {target:.2f}m\", fontsize=12)\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.suptitle(\"Sample Camera Images with Mean LiDAR Distance\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('sample_images.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: sample_images.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f26eb7",
   "metadata": {},
   "source": [
    "## Step 6: Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26873677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = ResNetRegressor(pretrained=True)\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"MODEL ARCHITECTURE\")\n",
    "print(\"=\"*50)\n",
    "print(\"ResNet18 (pretrained on ImageNet)\")\n",
    "print(\"Modified final layer: FC(512) → FC(256) → ReLU → Dropout(0.3) → FC(1)\")\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c380b9",
   "metadata": {},
   "source": [
    "## Step 7: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0919e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for images, targets in dataloader:\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    return total_loss / num_batches\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in dataloader:\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "    \n",
    "    mae = np.abs(all_preds - all_targets).mean()\n",
    "    rmse = np.sqrt(((all_preds - all_targets) ** 2).mean())\n",
    "    \n",
    "    ss_res = ((all_targets - all_preds) ** 2).sum()\n",
    "    ss_tot = ((all_targets - all_targets.mean()) ** 2).sum()\n",
    "    r2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'loss': total_loss / len(dataloader),\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'r2': r2,\n",
    "        'predictions': all_preds,\n",
    "        'targets': all_targets\n",
    "    }\n",
    "\n",
    "print(\"Training functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4630a57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "print(\"=\"*60)\n",
    "print(f\"TRAINING ResNet18 ({NUM_EPOCHS} epochs)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_maes = []\n",
    "best_val_loss = float('inf')\n",
    "best_epoch = 0\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Evaluate\n",
    "    val_results = evaluate(model, test_loader, criterion, device)\n",
    "    val_losses.append(val_results['loss'])\n",
    "    val_maes.append(val_results['mae'])\n",
    "    \n",
    "    # Track best\n",
    "    if val_results['loss'] < best_val_loss:\n",
    "        best_val_loss = val_results['loss']\n",
    "        best_epoch = epoch + 1\n",
    "        # Save best model\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    print(f\"Epoch {epoch+1:2d}/{NUM_EPOCHS} | \"\n",
    "          f\"Train Loss: {train_loss:.4f} | \"\n",
    "          f\"Val Loss: {val_results['loss']:.4f} | \"\n",
    "          f\"Val MAE: {val_results['mae']:.4f}m | \"\n",
    "          f\"Time: {epoch_time:.1f}s\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTraining completed in {total_time/60:.1f} minutes\")\n",
    "print(f\"Best epoch: {best_epoch} (Val Loss: {best_val_loss:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e68dff",
   "metadata": {},
   "source": [
    "## Step 8: Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629f26ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model and get final predictions\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "final_results = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"FINAL TEST RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"MAE:  {final_results['mae']:.4f} meters\")\n",
    "print(f\"RMSE: {final_results['rmse']:.4f} meters\")\n",
    "print(f\"R²:   {final_results['r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db25ae9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: Training History\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1 = axes[0]\n",
    "epochs = range(1, NUM_EPOCHS + 1)\n",
    "ax1.plot(epochs, train_losses, 'b-', label='Training Loss', linewidth=2, marker='o')\n",
    "ax1.plot(epochs, val_losses, 'r-', label='Validation Loss', linewidth=2, marker='s')\n",
    "ax1.axvline(x=best_epoch, color='green', linestyle='--', label=f'Best Epoch ({best_epoch})')\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss (MSE)', fontsize=12)\n",
    "ax1.set_title('Training and Validation Loss', fontsize=14)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# MAE curve\n",
    "ax2 = axes[1]\n",
    "ax2.plot(epochs, val_maes, 'g-', linewidth=2, marker='o')\n",
    "ax2.axvline(x=best_epoch, color='green', linestyle='--', label=f'Best Epoch ({best_epoch})')\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('MAE (meters)', fontsize=12)\n",
    "ax2.set_title('Validation MAE (Mean Absolute Error)', fontsize=14)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('ResNet18 Training Progress\\n(Predicting Mean LiDAR Distance from Camera Images)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: training_history.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a3f5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: Predictions Analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "targets = final_results['targets']\n",
    "predictions = final_results['predictions']\n",
    "errors = predictions - targets\n",
    "\n",
    "# Predicted vs Actual\n",
    "ax1 = axes[0, 0]\n",
    "ax1.scatter(targets, predictions, alpha=0.5, s=30, c='blue', edgecolors='navy')\n",
    "min_val = min(targets.min(), predictions.min())\n",
    "max_val = max(targets.max(), predictions.max())\n",
    "ax1.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect prediction')\n",
    "ax1.set_xlabel('Actual Mean Distance (m)', fontsize=12)\n",
    "ax1.set_ylabel('Predicted Mean Distance (m)', fontsize=12)\n",
    "ax1.set_title('Predicted vs Actual (Test Set)', fontsize=14)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Error distribution\n",
    "ax2 = axes[0, 1]\n",
    "ax2.hist(errors, bins=30, color='green', alpha=0.7, edgecolor='darkgreen')\n",
    "ax2.axvline(x=0, color='r', linestyle='--', lw=2, label='Zero error')\n",
    "ax2.axvline(x=errors.mean(), color='orange', linestyle='-', lw=2, label=f'Mean: {errors.mean():.3f}m')\n",
    "ax2.set_xlabel('Prediction Error (m)', fontsize=12)\n",
    "ax2.set_ylabel('Frequency', fontsize=12)\n",
    "ax2.set_title('Error Distribution', fontsize=14)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Residual plot\n",
    "ax3 = axes[1, 0]\n",
    "ax3.scatter(predictions, errors, alpha=0.5, s=30, c='purple', edgecolors='indigo')\n",
    "ax3.axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "ax3.set_xlabel('Predicted Distance (m)', fontsize=12)\n",
    "ax3.set_ylabel('Residual (Pred - Actual)', fontsize=12)\n",
    "ax3.set_title('Residual Plot', fontsize=14)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Timeline\n",
    "ax4 = axes[1, 1]\n",
    "frames = np.arange(len(targets))\n",
    "ax4.plot(frames, targets, 'b-', alpha=0.7, label='Actual', linewidth=1)\n",
    "ax4.plot(frames, predictions, 'r-', alpha=0.7, label='Predicted', linewidth=1)\n",
    "ax4.set_xlabel('Frame Index', fontsize=12)\n",
    "ax4.set_ylabel('Mean Distance (m)', fontsize=12)\n",
    "ax4.set_title('Prediction Timeline (Test Session - Mlab)', fontsize=14)\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'ResNet18 Test Results\\nMAE: {final_results[\"mae\"]:.4f}m | RMSE: {final_results[\"rmse\"]:.4f}m | R²: {final_results[\"r2\"]:.4f}', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('prediction_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: prediction_results.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71b6cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3: Sample Predictions with Images\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "\n",
    "# Get some test samples\n",
    "sample_indices = np.linspace(0, len(test_dataset)-1, 10, dtype=int)\n",
    "\n",
    "for idx, ax in zip(sample_indices, axes.flat):\n",
    "    img_path, actual = test_dataset.samples[idx]\n",
    "    predicted = predictions[idx]\n",
    "    error = abs(predicted - actual)\n",
    "    \n",
    "    img = Image.open(img_path)\n",
    "    ax.imshow(img)\n",
    "    \n",
    "    # Color based on error\n",
    "    color = 'green' if error < 0.1 else ('orange' if error < 0.3 else 'red')\n",
    "    ax.set_title(f\"Actual: {actual:.2f}m\\nPred: {predicted:.2f}m\\nError: {error:.2f}m\", \n",
    "                 fontsize=10, color=color)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Sample Predictions on Test Set (Green=Good, Orange=OK, Red=Poor)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('sample_predictions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: sample_predictions.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e664c53",
   "metadata": {},
   "source": [
    "## Step 9: Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57301f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\"\"\n",
    "DATASET INFORMATION:\n",
    "  - Source: Unitree Go1 Robot with RoboSense Helios-16 LiDAR\n",
    "  - Data: 100% REAL sensor measurements (NO synthetic data)\n",
    "  - Train: {len(train_dataset)} frames from 4 indoor sessions\n",
    "  - Test:  {len(test_dataset)} frames from 1 held-out session (Mlab)\n",
    "\n",
    "TASK:\n",
    "  - Cross-modal learning: Predict LiDAR depth from camera RGB images\n",
    "  - Input:  Camera image (resized to {IMAGE_SIZE}x{IMAGE_SIZE})\n",
    "  - Output: Mean distance of LiDAR points (meters)\n",
    "\n",
    "MODEL:\n",
    "  - Architecture: ResNet18 (pretrained on ImageNet)\n",
    "  - Fine-tuned head: FC(512) → FC(256) → ReLU → Dropout(0.3) → FC(1)\n",
    "  - Training: {NUM_EPOCHS} epochs, batch size {BATCH_SIZE}, Adam optimizer (lr={LEARNING_RATE})\n",
    "  - Training time: {total_time/60:.1f} minutes\n",
    "\n",
    "RESULTS:\n",
    "  - Best Epoch: {best_epoch}\n",
    "  - Test MAE:   {final_results['mae']:.4f} meters\n",
    "  - Test RMSE:  {final_results['rmse']:.4f} meters  \n",
    "  - Test R²:    {final_results['r2']:.4f}\n",
    "\n",
    "GENERATED PLOTS:\n",
    "  - sample_images.png (dataset visualization)\n",
    "  - training_history.png (loss curves)\n",
    "  - prediction_results.png (4-panel analysis)\n",
    "  - sample_predictions.png (visual predictions)\n",
    "\n",
    "INTERPRETATION:\n",
    "  - The model learns to predict scene depth from visual appearance\n",
    "  - This demonstrates cross-modal learning (Camera → LiDAR)\n",
    "  - Transfer learning from ImageNet provides useful visual features\n",
    "  - Train/test split by session tests generalization to new environments\n",
    "\"\"\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c7c73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all results\n",
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "# Create results zip\n",
    "result_files = ['sample_images.png', 'training_history.png', 'prediction_results.png', \n",
    "                'sample_predictions.png', 'best_model.pth']\n",
    "\n",
    "shutil.make_archive('resnet_results', 'zip', '.', '.')\n",
    "\n",
    "print(\"Downloading results...\")\n",
    "for f in result_files:\n",
    "    if os.path.exists(f):\n",
    "        files.download(f)\n",
    "        print(f\"  Downloaded: {f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
