# Quick Reference Guide - Unitree Go1 Dataset Collection

## üéØ What This Project Does (In 30 Seconds)

**Collects synchronized camera+LiDAR data from Unitree Go1 robot** ‚Üí **Implements IEEE paper algorithm for sensor sync** ‚Üí **Trains ResNet18 to predict depth from RGB images**

---

## üìä File-by-File Summary

### 1. **app.py** (459 lines) - Web Interface for Remote Data Collection
```
PURPOSE: Remote SSH control of robot sensors
ROBOT IPs: Camera=192.168.123.13 | LiDAR=192.168.123.15

START: python app.py
ACCESS: http://localhost:5000

FEATURES:
  ‚Ä¢ Record camera (ffmpeg v4l2) + LiDAR (tcpdump)
  ‚Ä¢ Simultaneous dual-sensor control
  ‚Ä¢ Automatic SFTP file transfer
  ‚Ä¢ Real-time status logs in web UI
  ‚Ä¢ Session-based organization

STATUS: ‚úÖ COMPLETE & WORKING
```

### 2. **paper_sync.py** (326 lines) - IEEE Algorithm 1 Implementation
```
PURPOSE: Adaptive time delay estimation for LiDAR-camera sync
PAPER: "Precise Synchronization..." IEEE TIV 2025 (Gurumadaiah et al.)

KEY ALGORITHM:
  For each frame pair:
    1. Project LiDAR ‚Üí Camera (Eq. 5)
    2. Compute projection error (pd) & time error (td)
    3. If error > threshold: Adjust sync offset

CLASS: PaperSync
MAIN: algorithm1(Ls_list, Cf_list, tL_list, tC_list, is_static_list)

STATUS: üü° FUNCTIONAL BUT NEEDS CALIBRATION
ISSUE: Uses placeholder calibration matrices (identity + hardcoded offsets)
```

### 3. **visualize_sync.py** (203 lines) - Validation Visualization
```
PURPOSE: Validate sensor sync by projecting LiDAR onto camera image
CONCEPT: If sync is good, LiDAR points should align with image edges

WORKFLOW:
  Load LiDAR .bin ‚Üí Filter points in front of camera
  ‚Üí Project to fisheye camera image ‚Üí Color by depth
  ‚Üí Render on camera frame

OUTPUT: PNG images with LiDAR dots overlaid

STATUS: ‚úÖ COMPLETE & WORKING
```

### 4. **lidar_camera_overlay.py** (247 lines) - Alternative Overlay Tool
```
PURPOSE: Same as visualize_sync.py but with batch processing
KEY METHOD: process_session(session_dir, num_frames=5)

STATUS: ‚úÖ COMPLETE
NOTE: Largely duplicates visualize_sync.py
```

### 5. **train_resnet_model.py** (295 lines) - ML Training Pipeline
```
PURPOSE: Train ResNet18 to predict mean LiDAR distance from camera RGB

TASK: Cross-modal learning
INPUT:  Camera image (224√ó224 RGB)
OUTPUT: Mean distance of LiDAR points (meters)

DATASET:
  Train: 576 frames from "4th_floor_hallway_20251206_132136"
  Test:  505 frames from "Mlab_20251207_112819" (held-out)

MODEL: ResNet18 (ImageNet pretrained)
  ‚îî‚îÄ Custom head: FC(512) ‚Üí ReLU ‚Üí Dropout(0.3) ‚Üí FC(1)

TRAINING:
  Batch: 32 | Epochs: 5 | LR: 0.001 (Adam)
  Augmentation: Flip + Color Jitter
  Metrics: MAE, RMSE, R¬≤

OUTPUT ARTIFACTS:
  ‚Ä¢ resnet_camera_lidar_model.pth (trained weights)
  ‚Ä¢ training_history.json (metrics)
  ‚Ä¢ resnet_training_history.png (loss curves)
  ‚Ä¢ resnet_predictions.png (pred vs actual)

STATUS: ‚úÖ COMPLETE & TRAINED
RESULTS: MAE ~0.3-0.5m, R¬≤ ~0.6-0.8
```

### 6. **CAP6415_ResNet_Training.ipynb** (25 cells) - Jupyter Notebook
```
PURPOSE: Cloud-compatible Colab notebook for training

FEATURES:
  ‚Ä¢ Dataset upload via Colab UI
  ‚Ä¢ Model definition & training loop
  ‚Ä¢ Evaluation & visualization
  ‚Ä¢ GPU support detection

STRUCTURE:
  1. Environment setup
  2. Dataset loading
  3. Model definition
  4. Training with early stopping
  5. Results visualization

STATUS: üü° FRAMEWORK READY
ISSUE: Not tested in actual Colab environment
TODO: Test in google.colab, verify all cells execute
```

### 7. **README.md** - Full Documentation
```
STATUS: ‚úÖ COMPLETE

COVERS:
  ‚Ä¢ Hardware specifications
  ‚Ä¢ Installation instructions
  ‚Ä¢ Usage examples
  ‚Ä¢ Data format details
  ‚Ä¢ Dataset statistics
  ‚Ä¢ LiDAR packet specifications
  ‚Ä¢ Synchronization strategy
  ‚Ä¢ Author info
```

---

## üî¥ Missing Critical Files

| File | Purpose | Impact |
|------|---------|--------|
| `extract_dataset_v2.py` | Convert raw MP4/PCAP ‚Üí KITTI format | **CRITICAL** |
| `create_combined_csv.py` | Consolidate labels to CSV | Medium |
| `show_sample_data.py` | Visualization demo | Low |
| `simple_model_demo.py` | Training example | Low |

**Note:** These are referenced in README but not in repo (likely in .gitignore or separate branch)

---

## üöÄ How to Run Each Module

### **1. Start Remote Data Collection**
```bash
# Terminal 1: Start web interface
python app.py

# Terminal 2: Open browser
http://localhost:5000

# Use web UI to:
# ‚Ä¢ Click "Camera Start" + "LiDAR Start"
# ‚Ä¢ Record for 1-2 minutes
# ‚Ä¢ Click "Camera Stop" + "LiDAR Stop"
# ‚Ä¢ Click "Camera Save" + "LiDAR Save"
# Files: ./dataset/camera/camera_*.mp4 and ./dataset/lidar/lidar_*.pcap
```

### **2. Validate Synchronization**
```bash
# Show LiDAR projection on camera frame
python visualize_sync.py

# Or use overlay tool
python lidar_camera_overlay.py
```

### **3. Test Algorithm 1 (Sync Adjustment)**
```bash
# Run synchronization algorithm
python paper_sync.py

# Outputs diagnostic images to model_results/sync_output/
```

### **4. Train ResNet Model**
```bash
# Requires: dataset_v2/ directory with session structure
python train_resnet_model.py

# Outputs:
# ‚Ä¢ model_results/resnet_camera_lidar_model.pth
# ‚Ä¢ model_results/training_history.json
# ‚Ä¢ model_results/*.png (plots)
```

### **5. Train in Google Colab**
```
1. Upload CAP6415_ResNet_Training.ipynb to Colab
2. Execute cells sequentially
3. Upload dataset_v2.zip when prompted
4. Run training
5. Download results
```

---

## üì¶ Data Format

### **Input Raw Data:**
```
./dataset/
‚îú‚îÄ‚îÄ camera/
‚îÇ   ‚îî‚îÄ‚îÄ camera_20251206_132136.mp4  # Video from ffmpeg
‚îî‚îÄ‚îÄ lidar/
    ‚îî‚îÄ‚îÄ lidar_20251206_132136.pcap  # Packets from tcpdump
```

### **Processed Dataset (dataset_v2/):**
```
dataset_v2/4th_floor_hallway_20251206_132136/
‚îú‚îÄ‚îÄ frames/
‚îÇ   ‚îú‚îÄ‚îÄ 000000.png  # Camera image (1856√ó800)
‚îÇ   ‚îú‚îÄ‚îÄ 000001.png
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ velodyne/
‚îÇ   ‚îú‚îÄ‚îÄ 000000.bin  # LiDAR points: [X,Y,Z,intensity,return_type] (Nx5 float32)
‚îÇ   ‚îú‚îÄ‚îÄ 000001.bin
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ timestamps.txt  # Unix timestamps per frame
‚îú‚îÄ‚îÄ calib.txt       # Calibration (placeholder)
‚îî‚îÄ‚îÄ README.md
```

### **Model Input:**
```
Camera image ‚Üí PIL Image (1856√ó800) 
            ‚Üí Resize to 224√ó224
            ‚Üí Normalize by ImageNet mean/std
            ‚Üí PyTorch tensor

LiDAR points ‚Üí Load .bin file
            ‚Üí Compute mean distance: sqrt(X¬≤ + Y¬≤ + Z¬≤).mean()
            ‚Üí Use as regression target (meters)
```

---

## üéõÔ∏è Configuration Reference

### **Hardware**
```
Camera:  Unitree Go1 (192.168.123.13) - Dual Fisheye 1856√ó800 @ 50 FPS
LiDAR:   RoboSense Helios-16 - 16 channels, UDP ports 6699/7788
         (192.168.123.15) - 10 Hz frequency, DUAL RETURN mode
```

### **Training Hyperparameters** (train_resnet_model.py)
```
BATCH_SIZE = 32
NUM_EPOCHS = 5
LEARNING_RATE = 0.001
WEIGHT_DECAY = 1e-4
EARLY_STOP_PATIENCE = 3
IMAGE_SIZE = 224√ó224
```

### **Sync Algorithm Parameters** (paper_sync.py)
```
pthr = 5.0 px       # Projection error threshold
tthr = 0.001 s      # Time error threshold (1 millisecond)
C_intrinsic = [[fx, 0, cx], [0, fy, cy], [0, 0, 1]]  # Camera matrix
R_t = [[r11, r12, r13, tx], [r21, r22, r23, ty], [r31, r32, r33, tz]]
```

---

## üìä Key Statistics

```
Dataset Size:
  ‚Ä¢ Sessions: 5
  ‚Ä¢ Total frames: 3,308
  ‚Ä¢ Total LiDAR points: 44,311,369
  ‚Ä¢ Points per frame: ~13,600 (dual return)
  ‚Ä¢ Duration: 5.5 minutes
  ‚Ä¢ Output FPS: 10 Hz

Training Results:
  ‚Ä¢ Train split: 576 frames
  ‚Ä¢ Test split: 505 frames
  ‚Ä¢ Best MAE: ~0.3-0.5 meters
  ‚Ä¢ Best R¬≤: ~0.6-0.8
  ‚Ä¢ Training time: ~5 minutes (GPU)
```

---

## üîß Common Issues & Solutions

### **Issue 1: SSH connection timeout to robot**
```
Symptom: "Connection timeout" when starting record
Solution: 
  ‚Ä¢ Check robot IPs in CONFIG dict
  ‚Ä¢ Verify SSH credentials
  ‚Ä¢ Ping robot: ping 192.168.123.13
```

### **Issue 2: FFmpeg not found on robot**
```
Symptom: "ffmpeg: command not found"
Solution:
  ‚Ä¢ Install on robot: sudo apt install ffmpeg
  ‚Ä¢ Or use pre-built remote binary
```

### **Issue 3: Dataset path not found for training**
```
Symptom: "No data found" when running train_resnet_model.py
Solution:
  ‚Ä¢ Verify dataset_v2/ exists at specified path
  ‚Ä¢ Check session folder names match TRAIN_SESSIONS
  ‚Ä¢ Ensure frames/ and velodyne/ subdirs exist
```

### **Issue 4: Model not converging**
```
Symptom: MAE not improving after epoch 1
Solution:
  ‚Ä¢ Check calibration is correct (not using placeholder values)
  ‚Ä¢ Verify data normalization
  ‚Ä¢ Try different learning rates (0.0001 or 0.01)
  ‚Ä¢ Check for outliers in LiDAR data
```

### **Issue 5: Sync visualization shows misaligned points**
```
Symptom: LiDAR dots don't align with image edges
Solution:
  ‚Ä¢ Recalibrate camera-LiDAR extrinsic parameters
  ‚Ä¢ Don't use placeholder calibration matrices
  ‚Ä¢ Generate checkerboard calibration dataset
  ‚Ä¢ Use calibration toolbox (OpenCV or specialized tools)
```

---

## üìö Key References

### **Paper**
- Title: "Precise Synchronization Between LiDAR and Multiple Cameras for Autonomous Driving: An Adaptive Approach"
- Authors: Gurumadaiah, Park, Lee, Kim, Kwon
- Published: IEEE TIV 2025, Vol. 10, No. 3
- DOI: 10.1109/TIV.2024.3444780
- Implementation: Algorithm 1 (paper_sync.py)

### **Datasets**
- KITTI Dataset (reference format)
- Our dataset: 5 indoor sessions, 3,308 frames, 44.3M points

### **Technologies**
- PyTorch, torchvision (ResNet)
- Paramiko (SSH)
- Flask (web framework)
- OpenCV, Numpy, PIL
- Scapy (PCAP parsing)

---

## ‚úÖ What's Working Now

```
‚úÖ Web interface for remote data collection
‚úÖ Camera recording (ffmpeg) + LiDAR capture (tcpdump)
‚úÖ Automatic file transfer (SFTP)
‚úÖ ResNet18 training pipeline
‚úÖ Model artifact saving/loading
‚úÖ IEEE Algorithm 1 implementation
‚úÖ Synchronization visualization tools
‚úÖ Comprehensive documentation

‚ö†Ô∏è Sync validation (needs calibration)
‚ö†Ô∏è Notebook framework (not tested in Colab)
‚ö†Ô∏è Algorithm 1 (designed but not real-time active)

‚ùå Hardware trigger control
‚ùå Data extraction scripts (missing)
‚ùå Real-time PTP synchronization
‚ùå Automatic camera calibration
```

---

## üéì Academic Context

**Course:** CAP6415 - Computer Vision (Fall 2025)  
**Institution:** University of Central Florida  
**Student:** Sai Surya Cherupally  
**Goal:** Cross-modal learning with real robot sensors + IEEE paper implementation

---

## üìû Quick Debug Checklist

- [ ] Can connect to robot via SSH?
- [ ] Is ffmpeg installed on camera device?
- [ ] Do PCAP files contain LiDAR data?
- [ ] Is dataset_v2/ structure correct?
- [ ] Are frames/ and velodyne/ directories populated?
- [ ] Is calibration data available (checkerboard)?
- [ ] Did model training complete without errors?
- [ ] Are prediction values reasonable (0-20m range)?

---

**Document Version:** 1.0  
**Last Updated:** January 20, 2025  
**Scope:** Complete project overview for CAP6415 dataset collection system
